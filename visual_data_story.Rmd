---
title: A deep dive into chess YouTube videos
output: html_document
---

```{=html}
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, error=FALSE)
```

```{js}
$(function() {
  $(".level2").css('visibility', 'hidden');
  $(".level2").first().css('visibility', 'visible');
  $(".container-fluid").height($(".container-fluid").height() + 300);
  $(window).on('scroll', function() {
    $('h2').each(function() {
      var h2Top = $(this).offset().top - $(window).scrollTop();
      var windowHeight = $(window).height();
      if (h2Top >= 0 && h2Top <= windowHeight / 2) {
        $(this).parent('div').css('visibility', 'visible');
      } else if (h2Top > windowHeight / 2) {
        $(this).parent('div').css('visibility', 'hidden');
      }
    });
  });
})
```

```{css}
body {
  background-color: #000000;
  color:            #ffffff;
  font-family:      "Helvetica Neue", Helvetica, sans-serif;
    margin:           0;
  padding:          1rem;
}

h1, h2, a {
  color: #E6DBBD;
}

.figcaption {display: none}
```

## Preface

Over the past few weeks, I’ve been logging chess YouTube videos I’ve watched using a Google Form, recording both metadata about the videos and general game information.

The goal was to identify patterns in the content I consume — to evaluate whether it’s an efficient use of my time, relevant to my own learning, and whether it aligns with the types of opponents I’m likely to face online.

As of May 15, 2025, I've logged 42 videos across 10 different YouTube channels.

Since I typically watch these videos while cooking or just before bed, I filled in the form retroactively using my YouTube watch history. This kept the logging process as frictionless as possible.

## Is My Time Well-Spent?

Since I struggle with time management, this data gave me a chance to test a couple of assumptions I already had:

1. The chess videos I watch are often quite long.
2. The timing of my logging likely aligns with project deadlines.

![Boxplot of video lengths by weekday, showing Wednesday as the most active day with long viewing times.](timestamp_plot.png)

As I expected, particularly when considering the retroactive logging, the observations registered are centred around Wednesday. Unsurprisingly, that's when the projects are due.

Regarding video length, there's not much data logged on Mondays, so let's focus on Tuesday through Thursday:

- Tuesdays and Wednesdays show a median around 35 minutes.
- Thursdays push closer to 50 minutes.

Wednesdays also show the most variation:

- A cluster of "shorter" videos around 25 minutes with the least spread.
- A second cluster between 50 and 75 minutes.
- Three outliers over 100 minutes!

This reflects the types of videos I tend to watch. The shorter ones are usually either single-game uploads or entertainment content, like a creator guessing their subscribers' ratings. The longer ones tend to be deeper dives into one or two games, combining live play with post-game analysis. As for the three videos over 100 minutes, all of them were uploaded by Eric Rosen across his two channels, which tracks, considering he frequently posts full-length Twitch streams.

Altogether, the videos total 2079 minutes, or over 34 hours spread roughly two months! Both of my assumptions seem to be correct, but I can't be too proud of that, even if the videos are often white noise...

So, let's look at what actually happens in the chess games, to see if, at least, the content lines up with the kinds of games I want to learn from.

## Who's Playing?

I'm currently rated around 2300 on Lichess, which puts me in a relatively high percentile of active players. Based on that, I expected the games I watch to feature players at or above that level.

To measure this, I calculated the average rating per game (i.e., between both players), and adjusted these to Lichess-equivalent values, since I’ll be comparing with the Lichess public database later on. 

Because converting across rating systems [isn't easy](https://lichess.org/page/rating-systems), I used rough estimates based on experience:

- +150 for Chess.com.
- +250 for FIDE.

Most of the games were from Chess.com. While I did consider over-the-board games (which use FIDE ratings) when deciding on the adjustment, all of the logged games were played online.

![Density plot of adjusted player ratings, peaking around 1600, with a dashed mean line at 1628.](rating_plot.png)

Even after adjustment, the average rating comes out to 1628, which is much lower than expected. The lowest-rated YouTuber I follow is around 2000 FIDE, and some, like Hikaru Nakamura, are grandmasters rated above 2700.

So, what gives?

Most of these creators regularly do rating speedruns, starting from low-rated accounts and climbing up, often from 400 to 2000. These videos are framed as instructional, though they’re usually played against much weaker opponents. In that context, the numbers make sense. But it does raise a question: how much can you learn from a game where one player is vastly stronger than the other? This is hard to answer, however.

It’s also worth noting that the logging period wasn’t during tournament season, when creators are more likely to upload their own FIDE games, played against similarly rated opponents. Going forward, I might want to seek out those types of videos instead of just watching whatever the algorithm suggests.

## What about public data?

With this difference in mind, I wanted to see how the games I watch compare to a broader, unbiased sample. Specifically, I looked at how Black responds to the most popular first move in my logged data (1. e4) and compared this to the [Lichess Database API](https://lichess.org/api#tag/Opening-Explorer). Lichess is the second-largest chess website, and most importantly, an open-source organisation!

The goal was to explore whether the openings and win rates in my YouTube content differ from what’s commonly played. My assumption going in was that both the YouTubers’ personal preferences and my own biases (in choosing what to watch) would skew the results in both play rate and win rate.

![Bar chart comparing Black's win rate for different responses to 1. e4 between form data and Lichess data.](win_rate_plot.png)

Here, 1... c5 appears close to the baseline, and both 1... e6 and the "Other" category have too few form entries to be relevant. Therefore, two openings stand out. These two show significantly higher win rates for Black compared to the Lichess data.

- 1... c6, which is explicitly favoured by GothamChess and ChessCenturion.
- 1... e5, played often by Eric Rosen and a member of Chessbrah.

This shows a clear effect of the content creator in our statistics.

The 1... c5 result confuses me, though. Most creators I follow play 1. e4 regularly, and I can't name one who favours 1... c5 specifically, so I'd expect more games resulting in wins for White. Maybe because 1... c5 is known as a complex, double-edged opening, many creators use it, but not as their main move. So they might have wins as Black that counterbalance this.

Let's see if play rate can clarify this.

![Bar chart comparing how often different responses to 1. e4 were played in the form data vs. Lichess data.](play_rate_plot.png)

The play rate of 1... c5 in the videos was much higher than in the Lichess data. This is probably because it's my favourite move by far, so I tend to seek out videos that feature it.

1... c6 also tracks, as mentioned earlier — it's a favourite of many creators.

Notably, Lichess shows a higher share of 1... e6 and “Other” responses. This could be due to the lower-rated player pool (compared to the creators' real ratings), whereas strong players tend to avoid rare lines. At the moment, 1... e6 is also slightly out of fashion, as newer AI engine trends have shifted preferences - though that’s a discussion for another day.

Interestingly, the “Other” category almost perfectly reflects what we’d expect from creators not playing those lines. Since they make up half of each game, the bar is nearly half the size of the corresponding Lichess value.

## Conclusion

We can see that the data from my observations differs significantly from what the account ratings might suggest. I can say with confidence that most of the games are played as part of speedruns — so while the rating labels are low, the actual gameplay comes from much stronger players. In learning terms, that’s definitely not the same as studying lower-rated games with less complexity. While it's clear that the games are instructional, we haven’t yet quantified how the learning value of these speedrun games compares to games played between evenly matched, high-level opponents.

Overall, this data leaves me with a clear takeaway: intentionality.

- I need to be more intentional in seeking out games that reflect my repertoire (like the 1... c5 example).
- I need to be more intentional in watching games played against strong opposition - where learning is clearer.
- But most importantly, I need to be more intentional with how I spend my time!